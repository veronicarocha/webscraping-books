# Book API - API Completa para Gest√£o e Recomenda√ß√£o de Livros

## üìñ Descri√ß√£o
API REST completa para consulta, gest√£o e recomenda√ß√£o de livros com sistema de machine learning integrado. Desenvolvida com arquitetura modular e escal√°vel, incluindo monitoramento em tempo real.
O projeto √© composto por um pipeline completo, que vai desde a ingest√£o e raspagem de dados (scraping) at√© a exposi√ß√£o dos dados via API p√∫blica, com monitoramento e m√©tricas em Streamlit.

## Dados produtivos ##

**URL DA APLICA√á√ÉO:**

https://web-production-962ea.up.railway.app

Apontando para rota espec√≠fica:
https://web-production-962ea.up.railway.app/api/v1/health


**URL DO DASHBOARD:**
https://streamlit-dashboard-production-f9ea.up.railway.app


## üèóÔ∏è Arquitetura
- **Backend**: Flask + Flask-RESTful
- **Database**: PostgreSQL (Produ√ß√£o) / SQLite (Desenvolvimento)
- **Autentica√ß√£o**: JWT com roles (admin e ml_engineer)
- **ML**: Sistema de recomenda√ß√µes
- **Monitoramento**: Dashboard Streamlit com analytics
- **Documenta√ß√£o**: Swagger/OpenAPI 
- **Deploy**: Railway
- **Logs**: Estruturados em JSON

## üöÄ Funcionalidades

### üìö Gest√£o de Livros
- Listagem paginada de livros
- Busca por t√≠tulo e categoria
- Detalhes completos por ID
- Filtros por faixa de pre√ßo
- Livros mais bem avaliados

### üîê Autentica√ß√£o & Seguran√ßa
- Login JWT com refresh tokens
- Rotas protegidas por role
- Sistema de usu√°rios (admin, ml_engineer)

### ü§ñ Machine Learning
- Features para modelos ML
- Dataset de treinamento
- Sistema de predi√ß√µes
- API para integra√ß√£o com modelos

### üìä Analytics & Monitoramento
- Estat√≠sticas gerais da cole√ß√£o
- M√©tricas por categoria
- Dashboard em tempo real
- Logs estruturados de performance

### üîß Utilidades
- Health check da API
- Trigger de scraping
- Documenta√ß√£o interativa Swagger

## üõ†Ô∏è Instala√ß√£o e Desenvolvimento

### 1. Clone o reposit√≥rio

Realize o clone do projeto:
```bash
git clone https://github.com/veronicarocha/webscraping-books
cd webscraping-books
```

### Recomenda√ß√µes ###
### VSCode

Editor de c√≥digo recomendado para o projeto: VSCode.

# Pr√© requisitos #

## Python
Tenha o python criado e configurado na maquina  (utilizado python-3.11.9)
https://www.python.org/
Na instala√ß√£o IMPORTANTE marcar "Add Python to PATH"

Verifique no terminal a versao instalada (Reinicie o terminal):

```python --version ```

## Node 
interessante tamb√©m instalar o Node para utilizar (quando e se necess√°rio) npm install xxxx


## Instalar Dependencias ##

Depois crie a venv e instale as dependencias do projeto (comandos no terminal):

```
python -m venv venv
source venv/bin/activate  # ou source venv\Scripts\activate 
pip install -r requirements.txt
```

### 2. Variaveis de ambiente
crie um arquivo na raiz do projeto .env com o conteudo abaixo: 

```bash
SECRET_KEY=minha-chave-super-secreta-local
JWT_SECRET_KEY=jwt-chave-secreta-local
DATABASE_URL=postgresql://postgres:senha@localhost:5432/bookapi
ML_ENGINEER_PASSWORD=senha_ml_local
```

### Pr√© requisito
Instalar o Docker e Postgres 

```
https://www.docker.com/products/docker-desktop/
ou
https://desktop.docker.com/win/main/amd64/Docker%20Desktop%20Installer.exe
```

### 3. Setup da base

Quando o docker estiver rodando, volte para o terminal e digite:

```
docker run --name postgres-bookapi -e POSTGRES_PASSWORD=senha -e POSTGRES_DB=bookapi -p 5432:5432 -d postgres:15

```
Para verificar se o postgres est√° rodando, execute o comando: 
```docker ps ```

Se o container estiver rodando corretamente, executar o setup:
```python scripts/setup_database.py ```

Ter√° um retorno tipo:

```
 python scripts/setup_database.py
 >>> Modo: desenvolvimento (local)
 >>> Database: PostgreSQL - localhost:5432
‚úÖ Tabelas criadas com sucesso!
‚úÖ Conex√£o com PostgreSQL testada!
üìä Total de livros no banco: 0
```

### 3. Iniciar Servi√ßos
Rode os comandos abaixo em terminais separados:

# Terminal 1 - API Book-API:

```bash
python run.py
# API: http://localhost:5000
# Docs: http://localhost:5000/apidocs

```
Esse comando tamb√©m verifica se a base est√° completa ou se precisa realizar o scraping dos dados ainda.

Ter√° um retorno como:

¬¥¬¥¬¥

 python run.py
 >>> Modo: desenvolvimento (local)
 >>> Database: PostgreSQL - localhost:5432
>>> Tabelas criadas
>>>  An√°lise da Base:
>>>  Livros: 1
>>>  Categorias: 1
>>> INICIANDO SCRAPING INTELIGENTE
>>>   Meta: 1000 livros
>>>   Limite: 15 minutos
>>>   Offset: 1 categorias
2025-11-04 23:16:09,068 - app.services.scraper - INFO -  >>> BookScraper inicializado com sucesso
2025-11-04 23:16:09,645 - app.services.scraper - INFO - Encontradas 50 categorias
>>>    üìÇ Categorias para processar: 49/50
>>> [2/50] Processando: Mystery
>>>    Tempo: 0.0min | Livros: 1/1000
2025-11-04 23:16:09,660 - app.services.scraper - INFO - Scraping categoria: Mystery
2025-11-04 23:16:09,660 - app.services.scraper - INFO - Scraping p√°gina: http://books.toscrape.com/catalogue/category/books/mystery_3/index.html
2025-11-04 23:16:13,875 - app.services.scraper - INFO - Scraping p√°gina: http://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
2025-11-04 23:16:17,027 - app.services.scraper - INFO - ‚úÖ Mystery: 32 livros coletados
>>> Mystery: +32 novos,0 existentes
>>> [3/50] Processando: Historical Fiction
>>>    Tempo: 0.2min | Livros: 33/1000
2025-11-04 23:16:18,116 - app.services.scraper - INFO - Scraping categoria: Historical Fiction
2025-11-04 23:16:18,116 - app.services.scraper - INFO ....

¬¥¬¥¬¥

E j√° ter√° acess√≠vel as rotas como :

http://localhost:5000/apidocs/

http://localhost:5000/api/v1/books?per_page=6


# Terminal 2 - Dashboard:
entrar na pasta do dashboard e instalar as dependencias do dashboard

cd dashboard

```pip install -r requirements.txt```


S√≥ depois subir a aplica√ß√£o em si.

```bash
cd dashboard
streamlit run app.py
# Dashboard: http://localhost:8501
```

### 4. Popular Banco de Dados

Caso a base nao tenha ficado como esperado (1000 livros), com os dados carregados:
Os scrits abaixo podem ser utilizados para "equalizar" a base

```bash
python scripts/concilia_scraping.py
ou
python scripts/complete_scraping.py 
```

### 5. üì° Uso da API
Pode fazer requests via terminal ou via Swagger para teste
```bash
curl http://localhost:5000/api/v1/health
```


### 6. Autentica√ß√£o
Algumas rotas s√£o protegidas por usu√°rio e senha 
por enquanto n√£o existe um sistema de cadastro de usu√°rios
basta usar o usu√°rio e senha registrados na aplica√ß√£o para gerar o token
e utilizar o token nas chamadas necess√°rias

```bash
curl -X POST http://localhost:5000/api/v1/auth/login \
  -H "Content-Type: application/json" \
  -d '{"username": "admin", "password": "senha-admin"}'
```


### 7. Dashboard de Monitoramento
Acesse http://localhost:8501 para visualizar:

    - M√©tricas em tempo real
    - Gr√°ficos de performance
    - Logs da API
    - Estat√≠sticas por endpoint


### 8. Plano Arquitetural ‚Äì Book API

<div>
 <img src="resourses/desenho-aplicacao.png" width="800" height="700" alt="desenho_arquitetura" /> 
</div>




## üß© Arquitetura do Sistema

| Componente | Descri√ß√£o |
|-------------|------------|
| **Scraper (Python)** | Executa a raspagem do site e coleta dados atualizados dos livros. Pode ser disparado manualmente ou automaticamente. |
| **ETL / Transforma√ß√£o** | Processa, limpa e estrutura os dados antes de salvar no banco de dados PostgreSQL. |
| **Banco de Dados (PostgreSQL)** | Armazena as informa√ß√µes dos livros (id, t√≠tulo, pre√ßo, rating, categoria, etc). |
| **API Flask** | Fornece endpoints RESTful para consulta, an√°lise e opera√ß√µes sobre os dados. |
| **Painel Streamlit** | Exibe m√©tricas de uso, estat√≠sticas de livros e monitora a sa√∫de das rotas da API. |


### 9. Documenta√ß√£o das rotas da API

Collection para testes no POSTMAN disponivel no projeto

# CAT√ÅLOGO DE ENDPOINTS
## üîß CORE ENDPOINTS
| M√©todo	| Rota 					| 	Descri√ß√£o |
|---------|---------------|------------|
| GET	| /api/v1/health			|Health check da API |
| POST	| /api/v1/scraping/trigger	|Disparar scraping manual|

## üìö BOOKS ENDPOINTS
| M√©todo	| Rota                    |	Descri√ß√£o                   |
|---------|------------------------|-----------------------------|
| GET |	/api/v1/books			      	| Listar todos os livros          |
| GET |	/api/v1/books/<int:id>		| Detalhes de um livro espec√≠fico |
| GET |	/api/v1/books/search		| Buscar livros                   |
| GET |	/api/v1/books/top-rated		| Livros mais bem avaliados       |
| GET |	/api/v1/books/price-range	| Livros por faixa de pre√ßo       |

## üè∑Ô∏è CATEGORIES ENDPOINTS
| M√©todo| 	Rota					| Descri√ß√£o						|
|---------|---------------|---------------------|
| GET	| /api/v1/categories		| Listar todas as categorias	|
| GET	| /api/v1/stats/categories	| Estat√≠sticas por categoria	|

## üìä STATS ENDPOINTS
| M√©todo |	Rota	        | Descri√ß√£o |
|---------|---------------|------------|
| GET	 | /api/v1/stats/overview	|Vis√£o geral das estat√≠sticas|

## üîê AUTH ENDPOINTS
| M√©todo| 	Rota				| Descri√ß√£o					|
|---------|---------------|------------|
| POST	| /api/v1/auth/login	| Login e obten√ß√£o de token |
| POST	| /api/v1/auth/refresh	| Refresh do token JWT      |

## ü§ñ ML ENDPOINTS
| M√©todo| 	Rota					| Descri√ß√£o						|
|---------|---------------|------------|
| GET	| /api/v1/ml/features		| Features para machine learning|
| GET	| /api/v1/ml/training-data	| Dados de treinamento			|
| GET	| /api/v1/ml/predictions	| Previs√µes do modelo			|

## üêõ DEBUG ENDPOINTS
| M√©todo| 	Rota				| Descri√ß√£o			|
|---------|---------------|------------|
| GET	| /api/v1/debug/logs	| Acessar logs da API  |


### 10 . Exemplos de chamadas com requests/responses

Para acesso √† documenta√ß√£o acesse: https://web-production-962ea.up.railway.app/apidocs/#/


## 11. Considera√ß√µes de Manuten√ß√£o e Escalabilidade
  - O scraping pode ser agendado via cron job ou Airflow (futuro).
  - Implementar um sistema de autentica√ß√£o JWT
  - A API √© desacoplada do processo de ingest√£o, podendo escalar separadamente.
  - O Streamlit pode ser hospedado independentemente, consumindo a API por HTTPS.
  - Logs e m√©tricas podem ser adicionados (Prometheus + Grafana).